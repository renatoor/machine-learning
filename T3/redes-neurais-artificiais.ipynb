{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "residential-rough",
   "metadata": {},
   "source": [
    "# Redes neurais artificiais\n",
    "\n",
    "As redes neurais artificiais são modelos computacionais inspirado no sistema nervoso central dos seres humanos, ou seja, neuronios conectados que emites sinais entre si. As redes neurais são muito utilizadas no aprendizado de máquina e reconhecimento de padrões. Uma rede neural é constituida por inúmeras camadas contendo neurônios interligados que realizando um processamento e propagam o resultado destes processamento para os outros neurônios afim de obter um resultado dado o que foi inserido como entrada para a rede neural. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-royal",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "super-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetBinaryClassifier, NeuralNetClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-spyware",
   "metadata": {},
   "source": [
    "## Redes MPL\n",
    "\n",
    "As redes MPL são rede neurais com mais de uma camada totalmente conectadas entre si. Afim de treinar uma rede MPL, será utilizado o conjunto de dados do histórico crediário oferecido aos clientes por uma instituição financeira. A rede neural deve ser capaz de prever se o cliente da instituição financeira irá pagar os não uma dívida dadas as informações do cliente. O problema proposto é um problema de classificação binário, ou o usuário irá pagar ou não a dívida, portanto a rede neural produz como resultado os valores 0 (não irá pagar a dívida) ou 1 (irá pagar a dívida) dadas as 11 característica que recebe como entrada. A arquitetura da rede modelada para este problema possui uma camada linear oculta que recebe como entrada as 11 características do conjunto de dados, produz 64 valores que são passados para a camada linear de saída que produz 1 valor como resultado. A camada oculta utiliza a função de ativação ReLU. Em geral, problemas de classificação binários utilizam a função _sigmoide_ para mapear os resultados em um valor entre 0 e 1. A função _sigmoide_ não está explicitamente definida no passo _forward_ na rede neural, no entanto, a função de perda utilizada _BCEWithLogitsLoss_ aplica a função _sigmoide_ juntamente com a classe _BCELoss_, portanto não é necessário definir a função _sigmoide_ após a computação da camada de saída da rede neural. Além disso, outros parâmetro definidos para o treinamento da rede neural são 200 iterações (parâmetros _max\\_epochs_) e taxa de aprendizado (parâmetro _lr_) de 0.01. Em seguida é apresentada a acurácia da rede neural utilizando a função _classification\\_report_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "capable-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/credtrain.txt', sep='\\t', header=None)\n",
    "data_test = pd.read_csv('data/credtest.txt', sep='\\t', header=None)\n",
    "\n",
    "X = np.array(data.iloc[:, :-1]).astype(np.float32)\n",
    "y = np.array(data.iloc[:, -1]).astype(np.float32)\n",
    "\n",
    "X_test = np.array(data_test.iloc[:, :-1]).astype(np.float32)\n",
    "y_test = np.array(data_test.iloc[:, -1])\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_norm = ss.fit_transform(X)\n",
    "X_test_norm = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minute-floating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91       349\n",
      "           1       0.81      0.96      0.88       228\n",
      "\n",
      "    accuracy                           0.90       577\n",
      "   macro avg       0.89      0.91      0.90       577\n",
      "weighted avg       0.91      0.90      0.90       577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(11, n)\n",
    "        self.fc2 = torch.nn.Linear(n, 1)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = NeuralNetBinaryClassifier(Net, max_epochs=200, lr=0.01, criterion=torch.nn.BCEWithLogitsLoss, verbose=0)\n",
    "net.fit(X_norm, y)\n",
    "y_predict = net.predict(X_test_norm)\n",
    "print(classification_report(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-marks",
   "metadata": {},
   "source": [
    "Para a otimização dos hiperparâmetros de treinamento da rede neural é utilizada a classe _GridSearch_ da biblioteca _Scikit Learn_, esta classe testa todas as combinações de hiperparâmetros para encontrar os melhores resultados. Foram definidas as taxas de aprendizado 0.1, 0.01 e 0.001, o número de iterações 10, 20, 100, 200, 500, e o número de valor de saída da camada oculta 64, 128 e 256. Em seguida pode ser observado os melhores valores encontrados pelo _GridSearch_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proprietary-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=<class 'skorch.classifier.NeuralNetBinaryClassifier'>[initialized](\n",
       "  module_=Net(\n",
       "    (fc1): Linear(in_features=11, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  ),\n",
       "),\n",
       "             param_grid={'lr': [0.1, 0.01, 0.001],\n",
       "                         'max_epochs': [10, 20, 100, 200, 500],\n",
       "                         'module__n': [64, 128, 256]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'lr': [0.1, 0.01, 0.001],\n",
    "    'max_epochs': [10, 20, 100, 200, 500],\n",
    "    'module__n': [64, 128, 256]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(net, params, refit=True, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "invalid-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros otimizados: {'lr': 0.01, 'max_epochs': 500, 'module__n': 128}\n",
      "Melhor score: 0.8866666666666667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       350\n",
      "           1       0.81      0.97      0.88       227\n",
      "\n",
      "    accuracy                           0.90       577\n",
      "   macro avg       0.89      0.91      0.90       577\n",
      "weighted avg       0.91      0.90      0.90       577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Parâmetros otimizados: {grid_search.best_params_}')\n",
    "print(f'Melhor score: {grid_search.best_score_}\\n')\n",
    "y_grid_predict = grid_search.predict(X_test_norm)\n",
    "print(classification_report(y_grid_predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-contamination",
   "metadata": {},
   "source": [
    "## Redes convolucionais\n",
    "\n",
    "Redes neurais convolucionais são um tipo de rede neural bastante utilizado para problemas de análises de imagens digitais. Nesta seção, serão apresentadas duas redes neurais: uma rede neural MPL e uma rede convolucional. Ambas as redes serão treinadas para identificar se uma imagem possui um gato ou não e em seguida será avaliar a acurácia das duas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-collapse",
   "metadata": {},
   "source": [
    "### Carga dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surprised-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('data/train_catvnoncat.h5', 'r' )\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'][:]).astype(np.float32) #your train set features\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'][:]) #your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('data/test_catvnoncat.h5', 'r' )\n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'][:]).astype(np.float32) #your test set features\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'][:]) #your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset['list_classes'][:]) #the list of classes\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "X_train, y_train, X_test, y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-lyric",
   "metadata": {},
   "source": [
    "### MPL\n",
    "\n",
    "As imagens do conjunto de dados possuem tamanho 64 $\\times$ 64 pixels, sendo cada pixel composto por 3 valores para representar a sua cor: vermelho (R), verde (G) e azul (B). Como redes neurais MPL não suportam dados tridimensionais como entrada deve fazer um preprocessamento nos e linearizar todos os valores em um único vetor, os dados linearizados são representados pelos vetores X_train_flatten e X_test_flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "extraordinary-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1) / 255\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-longitude",
   "metadata": {},
   "source": [
    "A arquitetura da rede neural MPL é composta por uma camada oculta que recebe como entrada 12288 valores ($ 64 \\times 64 \\times 3$), ou seja, o tamanho da imagem linearizada, a camada oculta produz 64 valores para a camada de saída que produz 2 valores. A camada oculta da rede utiliza a função de ativação ReLU. Para mapear os valores produzidos pela camada de saída entre 0 (imagem não possui um gato) e 1 (imagem possui um gato) deve-se aplicar a função _softmax_ para fazer o mapeamento. A função _softmax_ não é definida explicitamente na arquitetura da rede neural após a camada de saída no passo _forwards_, no entanto a função de perda definida pela classe _CrossEntropyLoss_ aplica a função _LogSoftmax_ juntamente com a função _LogSoftmax_. Após o treinamento da rede MPL é utilizada a função _classification\\_report_ para verificar a acurácia da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proper-madonna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.52      0.58        21\n",
      "           1       0.70      0.79      0.74        29\n",
      "\n",
      "    accuracy                           0.68        50\n",
      "   macro avg       0.67      0.66      0.66        50\n",
      "weighted avg       0.68      0.68      0.67        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NetSoftmax(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetSoftmax, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(64 * 64 * 3, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net_softmax = NeuralNetClassifier(NetSoftmax, max_epochs=250, lr=0.01, criterion=torch.nn.CrossEntropyLoss, verbose=0)\n",
    "net_softmax.fit(X_train_flatten, y_train)\n",
    "\n",
    "y_predict = net_softmax.predict(X_test_flatten)\n",
    "\n",
    "print(classification_report(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-diploma",
   "metadata": {},
   "source": [
    "### Rede convolucional\n",
    "\n",
    "Para carregar os dados do dataset na rede neural convolucional é necessário definir um dataset torch para refazer o preprocessamento dos dados. O dataset recebe como entrada o vetor de características X e os _targets_ y. Quando um valor é buscado no dataset torch a imagem que é representada por um array numpy é convertida para o formato de imagem da biblioteca _PIL_, esta imagem _PIL_ é então rotacionada, transformada em um tensor torch e os valores são normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pretty-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        X = Image.fromarray(np.uint8(X)).convert('RGB')\n",
    "        X = self.transform(X)\n",
    "        return X, y\n",
    "\n",
    "dataset = CustomDataset(X_train, y_train)\n",
    "dataset_test = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-rings",
   "metadata": {},
   "source": [
    "A arquitetura da rede convolucional consiste de uma camada convolucional, uma camada de _max pooling_ para fazer o _down-sample_ da imagem, uma camada linear oculta que recebe a imagem linearizada e produz 64 valores, e uma camada linear de saída que produz 2 valores. No passo _forward_ a camada convolucional utiliza a função de ativação ReLU, então é aplicado o _max pooling_ para fazer o _down-sample_ da imagem, em seguida a imagem é linearizada pela função _view_ para ser utilizada de entrada para a camada linear ocultada, que utiliza a função de ativacão ReLU, e por fim a imagem passa pela camada de saída. A rede convolucional é treinada com 50 iterações, taxa de aprendizado de 0.001, função de perda definida pela classe _CrossEntropyLoss_ e otimizador _Adam_. Em seguite é apresentada a acurácia da rede treinada utilizando a função _classification\\_report_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "collect-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 32, 32)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 3, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(18 * 32 * 32, 64)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (3, 32, 32) to (18, 32, 32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (18, 32, 32) to (18, 16, 16)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (18, 16, 16) to (1, 4608)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 18 * 32 * 32)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4608) to (1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "         \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 64) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "valued-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.68      0.77        22\n",
      "           1       0.79      0.93      0.85        28\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.84      0.81      0.81        50\n",
      "weighted avg       0.83      0.82      0.82        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_cnn = NeuralNetClassifier(\n",
    "    SimpleCNN,\n",
    "    max_epochs=50,\n",
    "    lr=0.001,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=None,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "net_cnn.fit(dataset, y_train)\n",
    "y_predict = net_cnn.predict(dataset_test)\n",
    "print(classification_report(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-restoration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
